# بسم الله الرحمن الرحيم
# الحمد لله، والصلاة والسلام على رسول الله ﷺ

---

## 1. Introduction to Data Engineering

### 1.1 What is Data Engineering?
Data Engineering is the discipline focused on designing, building, and maintaining the systems and architectures that enable the collection, storage, and processing of large volumes of data. It ensures that data is available in the right format, at the right time, and with sufficient quality to support analysis, decision-making, and (where relevant) machine learning models.

**Key Points**:
- Emphasis on scalability and reliability of data systems.  
- Concerned with data flow, from ingestion to transformation and storage.  
- Involves setting up pipelines that ingest raw data from multiple sources, transform it, and store it in efficient structures for further analysis.

### 1.2 How It Relates to Data Science
Where Data Science often focuses on deriving insights from data (e.g., building predictive models), Data Engineering provides the infrastructure to ensure the data used by data scientists is clean, consistent, and timely. Think of Data Engineers as the “plumbers” of data: they bring the data to the Data Scientists, ensuring everything runs reliably and at scale.

---

## 2. Data Engineering Components

### 2.1 Data Ingestion
Data ingestion is the process of loading data from various sources (databases, APIs, flat files, streaming services, etc.) into a system. Key factors include:
- **File Formats**: CSV, JSON, Parquet, Avro.
- **Protocols**: REST APIs, streaming (Kafka), direct database connections.
- **Batch vs. Streaming**: Batch ingestion processes data in chunks; streaming ingests data in real-time or near real-time.

#### Example:  
```python
import pandas as pd

# Reading CSV data from a local file
df_csv = pd.read_csv("sample_data.csv")

# Reading JSON data from a URL endpoint
url = "https://example.com/api/data.json"
df_json = pd.read_json(url)

# Display a few rows
print(df_csv.head())
print(df_json.head())
```

### 2.2 Data Storage and Management
Data Engineers work with various storage solutions:
1. **Data Warehouses** (e.g., Amazon Redshift, Google BigQuery): For structured, query-optimized data.
2. **Data Lakes** (e.g., AWS S3, Azure Data Lake Storage): For raw, unstructured, or semi-structured data.
3. **Databases** (Relational and NoSQL):
   - Relational (e.g., PostgreSQL, MySQL) for structured data.
   - NoSQL (e.g., MongoDB, Cassandra) for flexible or unstructured data.

Selecting the right storage depends on factors such as data structure, query patterns, and scalability requirements.

---

## 3. Data Cleaning and Preprocessing

### 3.1 Importance of Data Quality
Poor data quality leads to unreliable insights. Data cleaning ensures your dataset is consistent, complete, and free of errors.

**Common Tasks**:
- Handling missing values (imputation, removal).
- Removing duplicate rows.
- Resolving inconsistencies in data types.
- Addressing outliers (if required by domain constraints).

#### Example:
```python
# Assume df is a pandas DataFrame
# Drop rows with null values in specific columns
df.dropna(subset=["column_name"], inplace=True)

# Fill missing values in 'age' with the median
median_age = df["age"].median()
df["age"].fillna(median_age, inplace=True)

# Remove duplicate rows
df.drop_duplicates(inplace=True)
```

### 3.2 Data Transformation Techniques
- **Normalization**: Ensuring consistent units and ranges.  
- **Aggregation**: Summarizing data (e.g., daily to weekly).  
- **Feature Engineering** (for ML-based pipelines): Creating new features to capture domain-specific insights (e.g., date-time transformations, text processing).

---

## 4. Data Pipeline Orchestration

### 4.1 ETL vs. ELT
1. **ETL (Extract, Transform, Load)**: Data is extracted from sources, transformed to a clean/structured format, then loaded into a target system (e.g., a data warehouse).
2. **ELT (Extract, Load, Transform)**: Data is extracted from sources, loaded into a target system (often a data lake or warehouse), and transformed there.

**Key Consideration**:  
- ETL is traditionally associated with on-premise systems, while ELT is common with scalable cloud-based warehousing solutions.

### 4.2 Airflow, Luigi, and Other Orchestrators
Data pipeline orchestration is the automation of data workflows, which might include tasks such as:
- Scheduling data ingestion.
- Triggering transformations.
- Monitoring system health.

**Apache Airflow** is popular for creating DAGs (Directed Acyclic Graphs) that define and schedule a series of tasks (e.g., data ingestion, cleaning, transformation).  

```bash
# Example Airflow DAG structure (simplified, in Python pseudocode)

from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime

def extract_data():
    # Code to pull data from a source
    pass

def transform_data():
    # Code to clean/transform data
    pass

with DAG(dag_id='simple_etl',
         start_date=datetime(2025, 1, 1),
         schedule_interval='@daily') as dag:

    extract_task = PythonOperator(task_id='extract_data',
                                  python_callable=extract_data)
    
    transform_task = PythonOperator(task_id='transform_data',
                                    python_callable=transform_data)

    extract_task >> transform_task
```

*(Figure 1: A conceptual DAG where the “extract_data” task flows into the “transform_data” task.)*

---

## 5. Data Transformation and Processing Tools

### 5.1 Pandas
Pandas is a ubiquitous Python library for data manipulation and analysis, enabling:
- Fast operations on structured data (DataFrames).
- Read/Write to various file formats (CSV, Excel, JSON, Parquet).
- Powerful grouping and aggregation features.

**Example**:
```python
import pandas as pd

df = pd.read_csv("sales_data.csv")
grouped_sales = df.groupby("store_id")["amount"].sum().reset_index()
print(grouped_sales.head())
```

### 5.2 PySpark
For large-scale data, PySpark extends the capabilities of the Spark engine to Python:
- Distributed processing for big datasets.
- APIs for DataFrames, SQL queries, streaming, machine learning (Spark MLlib).
- Integration with HDFS, Hive, and other big data ecosystems.

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("PySparkExample").getOrCreate()
df_spark = spark.read.csv("large_data.csv", header=True, inferSchema=True)
df_spark.printSchema()

# Perform transformations
df_filtered = df_spark.filter(df_spark["amount"] > 1000)
df_filtered.show()
```

*(Figure 2: Conceptual diagram showing Spark clusters distributing data across worker nodes for parallel processing.)*

### 5.3 Dask
- Offers parallel computing on larger-than-memory datasets using familiar Pandas-like DataFrame API.
- Scales Python code to multiple cores or multiple machines.

---

## 6. Best Practices in Data Engineering

1. **Version Control**: Use Git to maintain versioning of code and configurations.
2. **Data Governance**: Implement policies for data access, retention, and compliance.
3. **Monitoring & Alerting**: Set up metrics (latency, throughput, failure rates) and alerts.
4. **Testing & CI/CD**: Automated testing of data pipelines ensures reliability of changes.
5. **Documentation**: Clear descriptions of pipeline steps, data schemas, and transformations.

---

## 7. Example End-to-End Workflow

Below is a simplified end-to-end data engineering workflow to illustrate how each component connects:

1. **Extract**: Load CSV files from an S3 bucket daily.  
2. **Transform**: Clean up missing data, filter rows based on a condition, and aggregate.  
3. **Load**: Store the transformed dataset into a PostgreSQL database for easy access.  
4. **Orchestrate**: Use Airflow to schedule the pipeline to run every night at 2 AM.  
5. **Monitor**: Configure email alerts if any step fails or if data volumes deviate significantly.

*(Figure 3: A conceptual pipeline diagram from S3 (extraction), Python-based transformations, to PostgreSQL (loading).)*

---

## 8. Example Code Snippet (Consolidated)

```python
import pandas as pd
import psycopg2
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime

def extract_data():
    # Example: Extract CSV from S3 (pseudo-code)
    df = pd.read_csv("s3://bucket/input_data.csv")
    df.to_csv("/tmp/extracted_data.csv", index=False)

def transform_data():
    df = pd.read_csv("/tmp/extracted_data.csv")
    df.dropna(subset=["key_col"], inplace=True)
    df["amount"].fillna(df["amount"].mean(), inplace=True)
    df_agg = df.groupby("category")["amount"].sum().reset_index()
    df_agg.to_csv("/tmp/transformed_data.csv", index=False)

def load_data():
    df_final = pd.read_csv("/tmp/transformed_data.csv")
    # Connect to a Postgres DB and load the data
    connection = psycopg2.connect(
        host="mydbhost",
        database="mydatabase",
        user="myuser",
        password="mypassword"
    )
    cursor = connection.cursor()
    
    # Create table if not exists (simplified)
    cursor.execute("""
    CREATE TABLE IF NOT EXISTS aggregated_data (
        category VARCHAR(50),
        amount NUMERIC
    );
    """)
    connection.commit()

    # Insert each row
    for _, row in df_final.iterrows():
        cursor.execute(
            "INSERT INTO aggregated_data (category, amount) VALUES (%s, %s)",
            (row["category"], row["amount"])
        )
    connection.commit()
    cursor.close()
    connection.close()

default_args = {
    'start_date': datetime(2025, 1, 1),
}

with DAG(dag_id='end_to_end_etl',
         default_args=default_args,
         schedule_interval='@daily') as dag:

    extract_task = PythonOperator(task_id='extract_data', python_callable=extract_data)
    transform_task = PythonOperator(task_id='transform_data', python_callable=transform_data)
    load_task = PythonOperator(task_id='load_data', python_callable=load_data)

    extract_task >> transform_task >> load_task
```

This example pipeline uses:
1. **Extract**: From S3.  
2. **Transform**: Pandas-based cleaning and aggregation.  
3. **Load**: Into a PostgreSQL table.  
4. **Orchestrate**: With Apache Airflow tasks for scheduling and monitoring.  

---

## 9. Review and Conclusion

Data Engineering is the backbone of any data-driven organization. By ensuring robust pipelines, consistent data quality, and scalable processing, Data Engineers empower Data Scientists to focus on analysis and modeling. This lecture highlighted:
1. Concepts of data ingestion, storage, and pipelines.  
2. Practical usage of Python tools like Pandas, PySpark, and orchestrators like Airflow.  
3. Best practices including testing, monitoring, and governance.  

Mastering these concepts will solidify one’s capacity to handle complex, real-world data scenarios efficiently and reliably.

---

## 10. Multiple-Choice Questions (MCQs)

Below are questions testing key concepts from this lecture. Correct answers are provided at the end.

### 10.1 Questions

**1.** Which step in a data pipeline typically involves removing duplicates, handling missing values, and fixing data types?  
A. Data Ingestion  
B. Data Preprocessing (Cleaning)  
C. Data Visualization  
D. Data Storage  

---

**2.** Which of the following best describes a key difference between ETL and ELT?  
A. ETL loads data without transforming it at any step.  
B. ELT transforms data before loading it into the warehouse.  
C. ETL transforms data before loading, whereas ELT often transforms data after loading.  
D. ETL is obsolete and no longer used in modern data systems.  

---

**3.** In the context of Data Engineering, which library is most appropriate for large-scale distributed processing?  
A. NumPy  
B. Pandas  
C. PySpark  
D. Matplotlib  

---

**4.** A Directed Acyclic Graph (DAG) in Airflow is used to:  
A. Store large amounts of raw data  
B. Perform machine learning on a single node  
C. Define and schedule tasks in a specific sequence without cyclical dependencies  
D. Visualize data using histograms and scatter plots  

---

**5.** In a Data Lake environment, which statement is true?  
A. Data is always strictly structured before storing it.  
B. It only supports relational tabular data.  
C. It can store raw, unstructured, or semi-structured data.  
D. Data Lakes must always replace Data Warehouses.  

---

**6.** Why might you choose a NoSQL database for certain data use cases?  
A. It strictly enforces rigid schema constraints.  
B. It is optimized only for joining multiple tables.  
C. It handles large volumes of rapidly changing, unstructured data more flexibly.  
D. It must be used if you have numerical data.  

---

**7.** Which of the following is considered a best practice in Data Engineering?  
A. Eliminating documentation to speed up development.  
B. Relying on manual pipeline runs rather than scheduling tasks.  
C. Maintaining version control and implementing CI/CD for data pipelines.  
D. Ignoring data quality issues to maintain pipeline speed.  

---

### 10.2 Answer Key

1. **B** – Data Preprocessing (Cleaning) addresses duplicates, missing values, and data type discrepancies.  
2. **C** – ETL transforms data before loading, whereas ELT typically loads raw data first, then transforms inside the target system.  
3. **C** – PySpark is specifically designed for distributed processing on large datasets.  
4. **C** – A DAG ensures a task sequence with no cycles, guiding how tasks depend on each other.  
5. **C** – Data Lakes can hold raw, unstructured, or semi-structured data, and do not necessarily replace warehouses.  
6. **C** – NoSQL databases allow flexible schemas, making them suitable for large, rapidly changing, or varied data.  
7. **C** – Adopting version control, testing, and CI/CD is a well-established best practice for reliability.

---

### Closing Remark
This lecture has laid out the fundamental concepts and practices for **Data Engineering** using Python, offering a solid reference for intermediate students. By thoughtfully applying these techniques and tools, you will build efficient, maintainable, and scalable data pipelines, paving the way for meaningful insights and analytics. Continue exploring real-world scenarios and experimenting with different libraries to deepen your mastery.
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7876,"sourceType":"datasetVersion","datasetId":5227},{"sourceId":2058462,"sourceType":"datasetVersion","datasetId":1233538}],"dockerImageVersionId":30822,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üìä Playing around with the housing DataSet","metadata":{}},{"cell_type":"markdown","source":"Dataset housing.csv The data contains information from the 1990 California census. So although it may not help you with predicting current housing prices like the Zillow Zestimate dataset, it does provide an accessible introductory dataset for teaching people about the basics of machine learning.\n\n1. longitude: A measure of how far west a house is; a higher value is farther west\n2. latitude: A measure of how far north a house is; a higher value is farther north\n3. housingMedianAge: Median age of a house within a block; a lower number is a newer building\n4. totalRooms: Total number of rooms within a block\n5. totalBedrooms: Total number of bedrooms within a block\n6. population: Total number of people residing within a block\n7. households: Total number of households, a group of people residing within a home unit, for a block\n8. medianIncome: Median income for households within a block of houses (measured in tens of thousands of US Dollars)\n9. medianHouseValue: Median house value for households within a block (measured in US Dollars)\n10. oceanProximity: Location of the house w.r.t ocean/sea","metadata":{}},{"cell_type":"code","source":"# Setting up the environment\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n\n\n# training, testing, hyperparameter tuning \nfrom sklearn.model_selection import train_test_split, GridSearchCV,  RandomizedSearchCV, cross_val_score, KFold\n\n# Feature scalers and transformers \nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.preprocessing import PowerTransformer, FunctionTransformer, OneHotEncoder \nfrom sklearn.compose import ColumnTransformer\n\n#Imputer\nfrom sklearn.impute import SimpleImputer\n\n\n# ML Algorithms\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, HistGradientBoostingRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom xgboost import XGBRegressor\n\n# feature selection\nfrom sklearn.feature_selection import SelectFromModel\n\n# Model evaluation metrics\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\n#Pipelines \nfrom sklearn.pipeline import Pipeline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-12T01:15:07.327218Z","iopub.execute_input":"2025-01-12T01:15:07.327702Z","iopub.status.idle":"2025-01-12T01:15:09.723987Z","shell.execute_reply.started":"2025-01-12T01:15:07.327667Z","shell.execute_reply":"2025-01-12T01:15:09.722659Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Outputs, please!\n%matplotlib inline\n\n# Seaborn & plt styles\nsns.set(style=\"whitegrid\")\nplt.style.use(\"ggplot\")\n\n# we don't want any warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T01:15:14.120204Z","iopub.execute_input":"2025-01-12T01:15:14.121082Z","iopub.status.idle":"2025-01-12T01:15:14.12985Z","shell.execute_reply.started":"2025-01-12T01:15:14.121035Z","shell.execute_reply":"2025-01-12T01:15:14.128415Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load and explore the dataset\ndf = pd.read_csv(\"/kaggle/input/california-housing-prices/housing.csv\")\ndf.head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T01:15:16.421868Z","iopub.execute_input":"2025-01-12T01:15:16.422342Z","iopub.status.idle":"2025-01-12T01:15:16.516859Z","shell.execute_reply.started":"2025-01-12T01:15:16.422309Z","shell.execute_reply":"2025-01-12T01:15:16.515525Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Let's see if there missing values\ndf.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T01:15:19.894226Z","iopub.execute_input":"2025-01-12T01:15:19.89472Z","iopub.status.idle":"2025-01-12T01:15:19.906232Z","shell.execute_reply.started":"2025-01-12T01:15:19.89468Z","shell.execute_reply":"2025-01-12T01:15:19.904986Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T01:15:23.169985Z","iopub.execute_input":"2025-01-12T01:15:23.170435Z","iopub.status.idle":"2025-01-12T01:15:23.197676Z","shell.execute_reply.started":"2025-01-12T01:15:23.1704Z","shell.execute_reply":"2025-01-12T01:15:23.196067Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Statistics\ndf.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T01:15:27.48585Z","iopub.execute_input":"2025-01-12T01:15:27.486308Z","iopub.status.idle":"2025-01-12T01:15:27.534596Z","shell.execute_reply.started":"2025-01-12T01:15:27.486262Z","shell.execute_reply":"2025-01-12T01:15:27.533323Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The dataset contains 20,640 rows and 10 columns.\nOnly one column is categorical (ocean_proximity), while the rest are numerical.\nThe total_bedrooms column has 207 missing values.\n\n**median_house_value** (target variable) has a maximum value of 500,001, which may indicate a truncated value or an imposed limit.\n**median_income ranges** from 0.49 to 15, with a mean of 3.87.\n**total_rooms and total_bedrooms** have very large maximum values, suggesting the presence of outliers.\n\nMissing Values\n\nThe **total_bedrooms** column has 207 missing values. ","metadata":{}},{"cell_type":"code","source":"# Histograms of numeric features\ndf.hist(figsize=(12, 8), bins=40, edgecolor='black', color='orange')\nplt.suptitle(\"Numeric feature distributions\", fontsize=16)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T01:15:30.514064Z","iopub.execute_input":"2025-01-12T01:15:30.514541Z","iopub.status.idle":"2025-01-12T01:15:32.988156Z","shell.execute_reply.started":"2025-01-12T01:15:30.514504Z","shell.execute_reply":"2025-01-12T01:15:32.98666Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* Skewness Analysis\nVariables with high positive skewness (right-skewed):\n\ntotal_rooms (4.15)\ntotal_bedrooms (3.46)\npopulation (4.94)\nhouseholds (3.41)\nmedian_income (1.65)\nmedian_house_value (0.98)\n\n‚ö†Ô∏è These variables have skewed distributions, which can affect the performance of regression models. ","metadata":{}},{"cell_type":"code","source":"# let's see featue value distrubutions and if there are some outliers\nplt.figure(figsize=(12, 8))\nfor i, column in enumerate(df.select_dtypes(include=['number']).columns, 1):\n    plt.subplot(3, 3, i)  \n    sns.boxplot(y=df[column], color='orange')\n    plt.title(f'{column}')\n\nplt.tight_layout()\nplt.show()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T01:15:34.246086Z","iopub.execute_input":"2025-01-12T01:15:34.246545Z","iopub.status.idle":"2025-01-12T01:15:35.857358Z","shell.execute_reply.started":"2025-01-12T01:15:34.246508Z","shell.execute_reply":"2025-01-12T01:15:35.856051Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Distributions**\n\n**total_rooms, total_bedrooms, population, and households** have highly right-skewed distributions.\n**median_income** has a more normal distribution but with a pronounced right tail.\n**median_house_value** has a strong peak at 500,001, confirming a possible artificial cap in the data.\n\n**Outliers**\n1. Extreme values are observed in **total_rooms, total_bedrooms, population, and households**.\n2. **median_income** also has some high values, but they are not as extreme.\n3. **median_house_value** shows a truncation at 500,001, which may impact regression models.","metadata":{}},{"cell_type":"code","source":"# Correlation matrix\ndf_hm = df.drop(columns = ['ocean_proximity'])\nplt.figure(figsize=(10, 6))\n\nmask = np.triu(np.ones_like(df_hm.corr()))\nsns.heatmap(df_hm.corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5, mask = mask)\nplt.title(\"Correlaion Matrix\", fontsize=14)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T01:15:39.300652Z","iopub.execute_input":"2025-01-12T01:15:39.301126Z","iopub.status.idle":"2025-01-12T01:15:39.895761Z","shell.execute_reply.started":"2025-01-12T01:15:39.301089Z","shell.execute_reply":"2025-01-12T01:15:39.894499Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**total_bedrooms** shows strong correlation to total_rooms \n\n**population** is correlated to : total_bedrooms and total_rooms\n\n**households** is also correlated with : total_rooms, total_bedrooms, population\n\n","metadata":{}},{"cell_type":"code","source":"# We want to see how strong is the correlation\n# Households and Population correlation chart\nplt.figure(figsize=(8, 4))\nsns.regplot(x=df[\"households\"], y=df[\"population\"], scatter_kws={\"alpha\": 0.5}, line_kws={\"color\": \"blue\"})\n\nplt.xlabel(\"Households number\")\nplt.ylabel(\"Population\")\nplt.title(\"Correlation between Households and Population\")\nplt.grid(True)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T01:15:43.942165Z","iopub.execute_input":"2025-01-12T01:15:43.942699Z","iopub.status.idle":"2025-01-12T01:15:45.419198Z","shell.execute_reply.started":"2025-01-12T01:15:43.942658Z","shell.execute_reply":"2025-01-12T01:15:45.417896Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Ok, but now we'd like to see how is households correlated to : total_rooms, total_bedrooms, population\ncorr_target_cols = [\"total_rooms\", \"total_bedrooms\", \"population\"]\nplt.figure(figsize=(12, 6))\n\nfor feature in corr_target_cols:\n    sns.regplot(x=df[\"households\"], y=df[feature], scatter_kws={\"alpha\": 0.5}, label=feature)\n\nplt.xlabel(\"Households\")\nplt.ylabel(\"total_rooms, total_bedrooms, population\")\nplt.title(\"Households correlatiom to: Total Rooms, Total Bedrooms and Population\")\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T01:15:48.66819Z","iopub.execute_input":"2025-01-12T01:15:48.668695Z","iopub.status.idle":"2025-01-12T01:15:53.114999Z","shell.execute_reply.started":"2025-01-12T01:15:48.668656Z","shell.execute_reply":"2025-01-12T01:15:53.113672Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Let's  QQ plot Numeric features\nnumerical_columns = df.select_dtypes(include=[np.number]).columns\n\n# Subcharts\nnum_vars = len(numerical_columns)\ncols = 3  \n\n# 3 cols and X rows grid chart\nrows = (num_vars // cols) + (num_vars % cols > 0)  \n\nplt.figure(figsize=(10, rows * 3))\n\n# QQ Plot for each numeric feature\nfor i, column in enumerate(numerical_columns, 1):\n    plt.subplot(rows, cols, i)\n    stats.probplot(df[column], dist=\"norm\", plot=plt)\n    plt.title(f\"{column}\")\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T01:15:58.049951Z","iopub.execute_input":"2025-01-12T01:15:58.050439Z","iopub.status.idle":"2025-01-12T01:16:00.850335Z","shell.execute_reply.started":"2025-01-12T01:15:58.0504Z","shell.execute_reply":"2025-01-12T01:16:00.849107Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# How features are correlated to target variable\nfeatures = [\"total_rooms\", \"total_bedrooms\", \"population\", \"households\", \"median_income\"]\n\nfor feature in features:\n    plt.figure(figsize=(10, 5))  \n    \n    sns.scatterplot(\n        x=df[feature], \n        y=df[\"median_house_value\"], \n        hue=df[\"median_house_value\"],  \n        size=df[\"median_house_value\"], \n        palette=\"viridis\",  \n        sizes=(10, 100),  \n        alpha=0.5  \n    )\n    \n    sns.regplot(\n        x=df[feature], \n        y=df[\"median_house_value\"], \n        scatter=False, \n        color=\"blue\",  \n        line_kws={\"linewidth\": 2} \n    )\n    \n    plt.xlabel(feature, fontsize=12)\n    plt.ylabel(\"Median House Value\", fontsize=12)\n    plt.title(f\"{feature} vs Median House Value\", fontsize=14, fontweight=\"bold\")  \n   \n    plt.show()  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T01:16:04.964908Z","iopub.execute_input":"2025-01-12T01:16:04.965359Z","iopub.status.idle":"2025-01-12T01:16:20.112768Z","shell.execute_reply.started":"2025-01-12T01:16:04.965324Z","shell.execute_reply":"2025-01-12T01:16:20.111149Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Remove NULL and NaN from df_heatmap\ndf_heatmap = df.replace([np.inf, -np.inf], np.nan )\ndf_heatmap.dropna(subset=[\"longitude\", \"latitude\", \"median_house_value\"], inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T02:09:48.296679Z","iopub.execute_input":"2025-01-12T02:09:48.297153Z","iopub.status.idle":"2025-01-12T02:09:48.31444Z","shell.execute_reply.started":"2025-01-12T02:09:48.29712Z","shell.execute_reply":"2025-01-12T02:09:48.312931Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# KDE Plot House price by locaton\n# Style \nsns.set_style(\"darkgrid\", {\"grid.color\": \".6\", \"grid.linestyle\": \":\"})\n\n# Figure\nplt.figure(figsize=(12, 8))\n\n# KdePlot\nsns.kdeplot(\n    x=df_heatmap[\"longitude\"], \n    y=df_heatmap[\"latitude\"], \n    weights=df_heatmap[\"median_house_value\"],  \n    cmap=\"coolwarm\",  \n    fill=True,  \n    thresh=0.02,  \n    levels=50,  \n    alpha=0.7  \n)\n\nplt.xlabel(\"Longitude\", fontsize=12)\nplt.ylabel(\"Latitude\", fontsize=12)\nplt.title(\"House price by locaton\", fontsize=14, fontweight=\"bold\")\nplt.show()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T02:09:53.051996Z","iopub.execute_input":"2025-01-12T02:09:53.052434Z","iopub.status.idle":"2025-01-12T02:10:10.0515Z","shell.execute_reply.started":"2025-01-12T02:09:53.052402Z","shell.execute_reply":"2025-01-12T02:10:10.050385Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#folium plugin\n!pip install folium","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T01:16:50.024304Z","iopub.execute_input":"2025-01-12T01:16:50.024767Z","iopub.status.idle":"2025-01-12T01:16:56.168649Z","shell.execute_reply.started":"2025-01-12T01:16:50.02473Z","shell.execute_reply":"2025-01-12T01:16:56.16714Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import folium\nfrom folium.plugins import HeatMap\nfrom branca.colormap import LinearColormap  \n\n# Default location (California)\ncenter_lat = df_heatmap[\"latitude\"].mean()\ncenter_lng = df_heatmap[\"longitude\"].mean()\n\n# Folium Maps\nfoliumfig = folium.Figure(width=1000, height=500)\nmapa = folium.Map(location=[center_lat, center_lng], zoom_start=7, tiles=\"cartodbpositron\").add_to(foliumfig)\n\n# Legend colors\ncolormap = LinearColormap(\n    colors=[\"blue\", \"green\", \"yellow\", \"orange\", \"red\"],  # Colors from minor to mayor price\n    vmin=df_heatmap[\"median_house_value\"].min(), \n    vmax=df_heatmap[\"median_house_value\"].max(),\n    caption=\"Median House Price ($)\"\n)\n\n# HeatMap based on median_house_value\nheat_data = list(zip(df_heatmap[\"latitude\"], df_heatmap[\"longitude\"], df_heatmap[\"median_house_value\"]))  \nHeatMap(heat_data, radius=15, blur=10, max_zoom=12, min_opacity=0.3).add_to(mapa)\n\n#Legend\ncolormap.add_to(mapa)\n\n# Here we go .... California Dreamin‚Äô!\nmapa","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T01:16:56.170652Z","iopub.execute_input":"2025-01-12T01:16:56.171062Z","iopub.status.idle":"2025-01-12T01:16:57.243346Z","shell.execute_reply.started":"2025-01-12T01:16:56.171022Z","shell.execute_reply":"2025-01-12T01:16:57.241951Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Copy the original DataFrame\ndf_trans = df.copy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T01:17:02.612112Z","iopub.execute_input":"2025-01-12T01:17:02.612859Z","iopub.status.idle":"2025-01-12T01:17:02.619671Z","shell.execute_reply.started":"2025-01-12T01:17:02.612814Z","shell.execute_reply":"2025-01-12T01:17:02.618415Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# I will check the histograms of feature columns to see how their values are distributed\ndf_trans.hist(bins=30, figsize=(15, 10), edgecolor='black', color='orange')\nplt.suptitle('Histograms of Numeric Columns', fontsize=16)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T01:17:06.684038Z","iopub.execute_input":"2025-01-12T01:17:06.68451Z","iopub.status.idle":"2025-01-12T01:17:09.420922Z","shell.execute_reply.started":"2025-01-12T01:17:06.684474Z","shell.execute_reply":"2025-01-12T01:17:09.419171Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Based on the histograms, we can draw the following conclusions about the variables:\n\n**median_income** : Shows right skewness (moderate positive skew).\n\n**total_rooms**, total_bedrooms, households: These variables have a right-skewed distribution (positively skewed). Their skewness appears to be stronger than that of median_income.\n\n**population** : Has a very high positive skew. Most values are concentrated near zero.\n\n**housing_median_age**: Its distribution appears closer to a normal distribution.\n\n**longitude & latitude**: Show a bimodal distribution, suggesting that they likely represent two main geographic zones with a higher concentration of houses or residential areas, such as highly populated cities or capital regions.","metadata":{}},{"cell_type":"code","source":"# Based on previous conclusions, we apply different transformations to skewed columns.\n# We create the DataFrame `df_trans` as a copy of `df_house`\ndf_trans = df.copy()\n\n# Columns to transform\n# We do not modify the columns `longitude` and `latitude` because they represent geographic coordinates\n# and do not show clear skewness in their distributions. They have defined ranges \n# and specific values that do not require normalization.\n\ncolumns_to_transform = ['total_rooms', 'total_bedrooms', 'population', 'households', 'median_income']\n\n# Power Transformer (Yeo-Johnson)\npower_transformer = PowerTransformer(method='yeo-johnson', standardize=False)\n\n# Apply transformation to numerical columns based on the magnitude of skewness \n# (strong skew or moderate skew)\nfor col in columns_to_transform:\n    if col == 'median_income':\n        # This column has a moderate skew, not as strong as the others. We use 'yeo-johnson'\n        df_trans[col] = power_transformer.fit_transform(df_trans[[col]])\n    else:\n        # Stronger skew: Logarithmic transformation (log1p):\n        df_trans[col] = np.log1p(df_trans[col])  # log1p to handle near-zero values\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T01:17:13.82082Z","iopub.execute_input":"2025-01-12T01:17:13.821249Z","iopub.status.idle":"2025-01-12T01:17:13.864732Z","shell.execute_reply.started":"2025-01-12T01:17:13.821216Z","shell.execute_reply":"2025-01-12T01:17:13.863469Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# How did it go?\nfig, axes = plt.subplots(len(columns_to_transform), 2, figsize=(9, 12))\nfor i, col in enumerate(columns_to_transform):\n    # Histograms of original\n    axes[i, 0].hist(df[col], bins=30, edgecolor='black', color='orange')\n    axes[i, 0].set_title(f'{col} (Original)')\n    axes[i, 0].set_ylabel('Frecuencia')\n\n    # Histograms of transformed\n    axes[i, 1].hist(df_trans[col], bins=30, edgecolor='black', color='orange')\n    axes[i, 1].set_title(f'{col} (Transformed)')\n\nplt.tight_layout()\nplt.suptitle('Comparison between  Original and Transformed', y=1.02, fontsize=16)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T01:17:19.599058Z","iopub.execute_input":"2025-01-12T01:17:19.599561Z","iopub.status.idle":"2025-01-12T01:17:22.609123Z","shell.execute_reply.started":"2025-01-12T01:17:19.599516Z","shell.execute_reply":"2025-01-12T01:17:22.607897Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# I treat missing values in column 'total_bedrooms', using   median strategy\nimputer = SimpleImputer(strategy=\"median\")\ndf_trans[\"total_bedrooms\"] = imputer.fit_transform(df_trans[[\"total_bedrooms\"]])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T01:17:36.409448Z","iopub.execute_input":"2025-01-12T01:17:36.409904Z","iopub.status.idle":"2025-01-12T01:17:36.426859Z","shell.execute_reply.started":"2025-01-12T01:17:36.409867Z","shell.execute_reply":"2025-01-12T01:17:36.425444Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# I Apply OneHot encoding to ocean_proximity\ndf_trans = pd.get_dummies(df_trans, columns=[\"ocean_proximity\"], drop_first=True, prefix=\"ocean_proximity\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T01:17:39.721995Z","iopub.execute_input":"2025-01-12T01:17:39.722437Z","iopub.status.idle":"2025-01-12T01:17:39.738205Z","shell.execute_reply.started":"2025-01-12T01:17:39.722405Z","shell.execute_reply":"2025-01-12T01:17:39.736908Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_trans.head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T01:17:41.780343Z","iopub.execute_input":"2025-01-12T01:17:41.780802Z","iopub.status.idle":"2025-01-12T01:17:41.805136Z","shell.execute_reply.started":"2025-01-12T01:17:41.780763Z","shell.execute_reply":"2025-01-12T01:17:41.803482Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Max values across not treated columns\ncol_names = ['total_rooms', 'total_bedrooms', 'population', 'households', 'median_income' ]\nmax_values_across = df_trans[col_names].max().max()\nmin_values_across = df_trans[col_names].min().min()\nprint(f'Min {min_values_across} Max {max_values_across}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T01:17:47.740908Z","iopub.execute_input":"2025-01-12T01:17:47.741377Z","iopub.status.idle":"2025-01-12T01:17:47.755933Z","shell.execute_reply.started":"2025-01-12T01:17:47.741339Z","shell.execute_reply":"2025-01-12T01:17:47.754488Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#  Min Max Scaler taking into account max_values_across\n# Define the columns to scale\nminmax_scaler_columns = ['longitude', 'latitude', 'housing_median_age']\n\n# Create MinMaxScaler with custom range\nminmax_scaler = MinMaxScaler(feature_range=(min_values_across, max_values_across))\n\n# Apply MinMaxScaler to the selected columns\ndf_trans[minmax_scaler_columns] = minmax_scaler.fit_transform(df_trans[minmax_scaler_columns])\n\ndf_trans.head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T01:17:51.508102Z","iopub.execute_input":"2025-01-12T01:17:51.508595Z","iopub.status.idle":"2025-01-12T01:17:51.536836Z","shell.execute_reply.started":"2025-01-12T01:17:51.508554Z","shell.execute_reply":"2025-01-12T01:17:51.535428Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Now, numerical features are on the same scale (more o less). Time to fiddle around with ML!","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Separation of features (X) and target variable (y) based on df_trans\n# Target variable\ny = df_trans[\"median_house_value\"]\n\n# Independent variables (all columns except \"median_house_value\")\nX = df_trans.drop(columns=[\"median_house_value\"])\n\n# Splitting into training and test sets (80% - 20%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T01:17:59.671035Z","iopub.execute_input":"2025-01-12T01:17:59.671552Z","iopub.status.idle":"2025-01-12T01:17:59.688895Z","shell.execute_reply.started":"2025-01-12T01:17:59.67151Z","shell.execute_reply":"2025-01-12T01:17:59.68725Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Linear Regression Model\nlinear_model = LinearRegression()\nlinear_model.fit(X_train, y_train)\n\n# Predictions on test\ny_pred = linear_model.predict(X_test)\n\n# Evaluacion del model\nmae = mean_absolute_error(y_test, y_pred)\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\nrmse = np.sqrt(mse)\n\nprint(f\"\\n Linear regression evaluation  :\")\nprint(f\"MAE: {mae:.2f}\")\nprint(f\"MSE: {mse:.2f}\")\nprint(f\"RMSE: {rmse:.2f}\")\nprint(f\"R¬≤ Score: {r2:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T01:18:03.812307Z","iopub.execute_input":"2025-01-12T01:18:03.812756Z","iopub.status.idle":"2025-01-12T01:18:03.865035Z","shell.execute_reply.started":"2025-01-12T01:18:03.812719Z","shell.execute_reply":"2025-01-12T01:18:03.863006Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Interpretation of Results**\n\n- If the R¬≤ Score is high (~0.7 or more), it means that Linear Regression predicts well.\n\n- If MAE and RMSE are low, the model has less error.\n- If the graph shows similar lines, the model fits the data well.\n- If there is a big difference between actual and predicted values, Linear Regression is not sufficient, and Random Forest or XGBoost is recommended.\n\n\n**In our case, the model follows the price trend but does not perform well. Therefore, we will use more advanced regression algorithms.**","metadata":{}},{"cell_type":"code","source":"# Let's see how well our model performs\n# Sorting the true values for better visualization\nsorted_idx = np.argsort(y_test)\ny_test_sorted = y_test.iloc[sorted_idx]\ny_pred_sorted = y_pred[sorted_idx]\n\n# The plot\nplt.figure(figsize=(18, 6))\n\n# Blue line: Actual values\nplt.plot(y_test_sorted.values, label=\"Actual\", color='blue')\n\n# Red line: Predicted values\nplt.plot(y_pred_sorted, label=\"Predicted\", color='red', linestyle='dashed', alpha=0.5)\n\n# Graph styling\nplt.xlabel(\"\")\nplt.ylabel(\"House Value ($)\")\nplt.title(\"Actual vs Predicted Values - Linear Regression\")\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T01:18:28.407229Z","iopub.execute_input":"2025-01-12T01:18:28.407736Z","iopub.status.idle":"2025-01-12T01:18:28.848865Z","shell.execute_reply.started":"2025-01-12T01:18:28.407702Z","shell.execute_reply":"2025-01-12T01:18:28.847511Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Lets see how the other algorithm perform\nmodels = {\n    \"Linear Regression\": LinearRegression(),\n    \"Ridge Regression\": Ridge(),\n    \"Lasso Regression\": Lasso(),\n    \"ElasticNet\": ElasticNet(),\n    \"Decision Tree\": DecisionTreeRegressor(random_state=42),\n    \"Random Forest\": RandomForestRegressor(random_state=42),\n    \"Extra Trees\": ExtraTreesRegressor(random_state=42),\n    \"Gradient Boosting\": GradientBoostingRegressor(random_state=42),\n    \"Hist Gradient Boosting\": HistGradientBoostingRegressor(random_state=42),\n    \"KNN Regressor\": KNeighborsRegressor()\n}\nresults = []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T01:22:00.011511Z","iopub.execute_input":"2025-01-12T01:22:00.011998Z","iopub.status.idle":"2025-01-12T01:22:00.019067Z","shell.execute_reply.started":"2025-01-12T01:22:00.011966Z","shell.execute_reply":"2025-01-12T01:22:00.017415Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ML model evaluation\nfor model_name, model in models.items():\n    print(f\"\\nüîç Evaluating Model: {model_name}...\")\n\n    scores = cross_val_score(model, X_train, y_train, cv=3, scoring=\"r2\", n_jobs=-1)\n\n    # Save  results\n    results.append({\n        \"model\": model_name,\n        \"Mean R¬≤ Score (CV)\": np.mean(scores),\n        \"Std R¬≤ Score (CV)\": np.std(scores)\n    })\n\n# results to df\ndf_results = pd.DataFrame(results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T01:23:54.872892Z","iopub.execute_input":"2025-01-12T01:23:54.873348Z","iopub.status.idle":"2025-01-12T01:24:19.771382Z","shell.execute_reply.started":"2025-01-12T01:23:54.873314Z","shell.execute_reply":"2025-01-12T01:24:19.769859Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Order by  median R¬≤ Score\ndf_results_sorted = df_results.sort_values(by=\"Mean R¬≤ Score (CV)\", ascending=False)\n\nplt.figure(figsize=(10, 6))\nplt.barh(df_results_sorted[\"model\"], df_results_sorted[\"Mean R¬≤ Score (CV)\"], color='blue', alpha=0.7)\n\nplt.xlabel(\"Mean R¬≤ Score (CV)\")\nplt.ylabel(\"Model\")\nplt.title(\"Model Performance (higher - is better)\")\nplt.gca().invert_yaxis()  \nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T01:28:31.986522Z","iopub.execute_input":"2025-01-12T01:28:31.986974Z","iopub.status.idle":"2025-01-12T01:28:32.417559Z","shell.execute_reply.started":"2025-01-12T01:28:31.986941Z","shell.execute_reply":"2025-01-12T01:28:32.416294Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# We test XGBRegressor separately because XGBRegressor and cross_val_score do not work well together\nX_scaled_kfold = X.to_numpy()\n\n# Convert 'y' to an array to avoid issues with iloc\ny_array = np.array(y)\n\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\nr2_scores = []\n\n# Initialize the XGBoost model (with default parameters)\nxgb_model = XGBRegressor(random_state=42, tree_method=\"hist\")\n\n# Iterate over the folds without overwriting main variables\nfor train_idx, val_idx in kf.split(X_scaled_kfold):\n    X_train_fold, X_val_fold = X_scaled_kfold[train_idx], X_scaled_kfold[val_idx]\n    y_train_fold, y_val_fold = y_array[train_idx], y_array[val_idx]\n\n    # Train the model on the current fold\n    xgb_model.fit(X_train_fold, y_train_fold)\n\n    # Get predictions on the validation set\n    y_pred = xgb_model.predict(X_val_fold)\n\n    # Calculate R¬≤ Score for this fold\n    r2 = r2_score(y_val_fold, y_pred)\n    r2_scores.append(r2)\n\n# Final results\nmean_r2 = np.mean(r2_scores)\nstd_r2 = np.std(r2_scores)\nprint(f\"\\nüìä Final Results:\")\nprint(f\"Mean R¬≤ Score (CV): {mean_r2:.4f}\")\nprint(f\"Std of R¬≤ Score (CV): {std_r2:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T01:45:33.667228Z","iopub.execute_input":"2025-01-12T01:45:33.66767Z","iopub.status.idle":"2025-01-12T01:45:34.96321Z","shell.execute_reply.started":"2025-01-12T01:45:33.667639Z","shell.execute_reply":"2025-01-12T01:45:34.960515Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"¬°Excellent! üöÄ XGBoost with manual cross-validation achieved a Mean R¬≤ Score (CV) = 0.83, with a standard deviation (Std R¬≤ Score) of only 0.01. üéØ\n\nüìå What does this mean?\n\n‚úî XGBoost has high performance (R¬≤ = 0.83), indicating that it explains the variability of the data well.\n\n‚úî The low standard deviation (0.01) means the model is stable and consistent across different data subsets.\n\n‚úî We can keep it as is or try to optimize it further by tuning hyperparameters (n_estimators, learning_rate, max_depth, etc.).","metadata":{}},{"cell_type":"code","source":"# XGBRegressor hyperparameter tuning  with grid search\n# Define the hyperparameters to tune\nparam_grid = {\n    \"n_estimators\": [120, 150, 200],  # Number of trees\n    \"learning_rate\": [0.05, 0.07, 0.1],  # Learning rate\n    \"max_depth\": [3, 10],  # Maximum tree depth\n    \"subsample\": [0.6, 0.8, 1.0],  # Percentage of data used for each tree\n    \"colsample_bytree\": [0.6, 0.8, 1.0]  # Percentage of features used per tree\n}\n\nbest_score = -np.inf\nbest_params = {}\n\n# Iterate over all combinations of hyperparameters\nfor n in param_grid[\"n_estimators\"]:\n    for lr in param_grid[\"learning_rate\"]:\n        for depth in param_grid[\"max_depth\"]:\n            for subsample in param_grid[\"subsample\"]:\n                for colsample in param_grid[\"colsample_bytree\"]:\n\n                    # Define the model with the current hyperparameters\n                    xgb_model = XGBRegressor(\n                        n_estimators=n, learning_rate=lr, max_depth=depth,\n                        subsample=subsample, colsample_bytree=colsample,\n                        random_state=42, tree_method=\"hist\"\n                    )\n\n                    # Train and evaluate\n                    xgb_model.fit(X_train, y_train)\n                    y_pred = xgb_model.predict(X_test)\n                    r2 = r2_score(y_test, y_pred)\n\n                    # Save the best hyperparameters\n                    if r2 > best_score:\n                        best_score = r2\n                        best_params = {\n                            \"n_estimators\": n,\n                            \"learning_rate\": lr,\n                            \"max_depth\": depth,\n                            \"subsample\": subsample,\n                            \"colsample_bytree\": colsample\n                        }\n\nprint(\"\\nüìå Best Hyperparameters Found:\", best_params)\nprint(f\"üìä Best R¬≤ Score on Test: {best_score:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T01:56:41.412751Z","iopub.execute_input":"2025-01-12T01:56:41.41324Z","iopub.status.idle":"2025-01-12T01:59:08.142775Z","shell.execute_reply.started":"2025-01-12T01:56:41.413203Z","shell.execute_reply":"2025-01-12T01:59:08.141662Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Let's see how our model xgb_model performs by plotting the results\nxgb_model = XGBRegressor(n_estimators=200, learning_rate=0.07, max_depth=10,\n                         subsample=1, colsample_bytree=0.6,\n                         random_state=42, tree_method=\"hist\")\n\nxgb_model.fit(X_train, y_train)\ny_pred = xgb_model.predict(X_test)\n\nsorted_idx = np.argsort(y_test)\ny_test_sorted = y_test.iloc[sorted_idx]\ny_pred_sorted = y_pred[sorted_idx]\n\n# The plot\nplt.figure(figsize=(18, 6))\n\n# Blue line: Actual values\nplt.plot(y_test_sorted.values, label=\"Actual Values\", color='blue')\n\n# Red line: Predicted values\nplt.plot(y_pred_sorted, label=\"Predicted Values (XGBRegressor)\", color='red', linestyle='dashed', alpha=0.5)\n\n# Chart styling\nplt.xlabel(\"Observation Index\")\nplt.ylabel(\"House Value ($)\")\nplt.title(\"Actual vs Predicted Values - XGBRegressor\")\nplt.legend()\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T02:02:13.800722Z","iopub.execute_input":"2025-01-12T02:02:13.801187Z","iopub.status.idle":"2025-01-12T02:02:15.910028Z","shell.execute_reply.started":"2025-01-12T02:02:13.801152Z","shell.execute_reply":"2025-01-12T02:02:15.9087Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Feature importance\nfeature_importance = xgb_model.feature_importances_\n\n# We create a DataFrame for feature importance\nfeature_importance_df = pd.DataFrame({\n    \"Feature\": X_train.columns,\n    \"Importance\": feature_importance\n}).sort_values(by=\"Importance\", ascending=False)\n\n#  Feature Importance Plot\nplt.figure(figsize=(12, 6))\nplt.barh(feature_importance_df[\"Feature\"], feature_importance_df[\"Importance\"], color='skyblue')\nplt.xlabel(\"Importance Score\")\nplt.ylabel(\"Feature\")\nplt.title(\"Feature Importance - XGBRegressor\")\nplt.gca().invert_yaxis()  # Highest importance at the top\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T02:05:41.512584Z","iopub.execute_input":"2025-01-12T02:05:41.513023Z","iopub.status.idle":"2025-01-12T02:05:41.92971Z","shell.execute_reply.started":"2025-01-12T02:05:41.512993Z","shell.execute_reply":"2025-01-12T02:05:41.928487Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}}]}